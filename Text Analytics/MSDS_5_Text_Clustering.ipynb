{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import iglob #function returns an interator (data is not stored in memory at the same time)\n",
    "#of files in a directory using shell pattern matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Health-Tweets\\\\bbchealth.txt',\n",
       " 'Health-Tweets\\\\cbchealth.txt',\n",
       " 'Health-Tweets\\\\cnnhealth.txt',\n",
       " 'Health-Tweets\\\\everydayhealth.txt',\n",
       " 'Health-Tweets\\\\foxnewshealth.txt',\n",
       " 'Health-Tweets\\\\gdnhealthcare.txt',\n",
       " 'Health-Tweets\\\\goodhealth.txt',\n",
       " 'Health-Tweets\\\\KaiserHealthNews.txt',\n",
       " 'Health-Tweets\\\\latimeshealth.txt',\n",
       " 'Health-Tweets\\\\msnhealthnews.txt',\n",
       " 'Health-Tweets\\\\NBChealth.txt',\n",
       " 'Health-Tweets\\\\nprhealth.txt',\n",
       " 'Health-Tweets\\\\nytimeshealth.txt',\n",
       " 'Health-Tweets\\\\reuters_health.txt',\n",
       " 'Health-Tweets\\\\usnewshealth.txt',\n",
       " 'Health-Tweets\\\\wsjhealth.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List all text files from the Health-Tweets folder\n",
    "file_list = list(iglob('Health-Tweets/*.txt'))\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health-Tweets\\bbchealth.txt\n",
      "Health-Tweets\\cbchealth.txt\n",
      "Health-Tweets\\cnnhealth.txt\n",
      "Health-Tweets\\everydayhealth.txt\n",
      "Health-Tweets\\foxnewshealth.txt\n",
      "Health-Tweets\\gdnhealthcare.txt\n",
      "Health-Tweets\\goodhealth.txt\n",
      "Health-Tweets\\KaiserHealthNews.txt\n",
      "Health-Tweets\\latimeshealth.txt\n",
      "Health-Tweets\\msnhealthnews.txt\n",
      "Health-Tweets\\NBChealth.txt\n",
      "Health-Tweets\\nprhealth.txt\n",
      "Health-Tweets\\nytimeshealth.txt\n",
      "Health-Tweets\\reuters_health.txt\n",
      "Health-Tweets\\usnewshealth.txt\n",
      "Health-Tweets\\wsjhealth.txt\n"
     ]
    }
   ],
   "source": [
    "# Find files using regular wildcards and iterate over them\n",
    "for file in iglob('Health-Tweets/*.txt'):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas dataframe (built on NumPy) \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(all_files):\n",
    "    \"\"\"Reads text files and combines them in one dataframe\"\"\"\n",
    "    for file_ in file_list:\n",
    "        df = pd.concat((pd.read_csv(f,                                                  \n",
    "                        sep='|',                                  # Use | as a separator \n",
    "                        header=None,                              # no header\n",
    "                        names=['id', 'datetime', 'tweet'],        # three fields   \n",
    "                        error_bad_lines=False,                    #ignore innapropriately formated lines\n",
    "                        encoding='latin-1')) for f in all_files)  # use latin - 1 encoding \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27738, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the first 8 files (half of the data set) and output as one data frame\n",
    "df = read_files(file_list[:7])\n",
    "df.shape #check size of the data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>585978391360221184</td>\n",
       "      <td>Thu Apr 09 01:31:50 +0000 2015</td>\n",
       "      <td>Breast cancer risk test devised http://bbc.in/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>585947808772960257</td>\n",
       "      <td>Wed Apr 08 23:30:18 +0000 2015</td>\n",
       "      <td>GP workload harming care - BMA poll http://bbc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>585947807816650752</td>\n",
       "      <td>Wed Apr 08 23:30:18 +0000 2015</td>\n",
       "      <td>Short people's 'heart risk greater' http://bbc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>585866060991078401</td>\n",
       "      <td>Wed Apr 08 18:05:28 +0000 2015</td>\n",
       "      <td>New approach against HIV 'promising' http://bb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>585794106170839041</td>\n",
       "      <td>Wed Apr 08 13:19:33 +0000 2015</td>\n",
       "      <td>Coalition 'undermined NHS' - doctors http://bb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>585733482413891584</td>\n",
       "      <td>Wed Apr 08 09:18:39 +0000 2015</td>\n",
       "      <td>Review of case against NHS manager http://bbc....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>585733481608646657</td>\n",
       "      <td>Wed Apr 08 09:18:39 +0000 2015</td>\n",
       "      <td>VIDEO: 'All day is empty, what am I going to d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                        datetime  \\\n",
       "0  585978391360221184  Thu Apr 09 01:31:50 +0000 2015   \n",
       "1  585947808772960257  Wed Apr 08 23:30:18 +0000 2015   \n",
       "2  585947807816650752  Wed Apr 08 23:30:18 +0000 2015   \n",
       "3  585866060991078401  Wed Apr 08 18:05:28 +0000 2015   \n",
       "4  585794106170839041  Wed Apr 08 13:19:33 +0000 2015   \n",
       "5  585733482413891584  Wed Apr 08 09:18:39 +0000 2015   \n",
       "6  585733481608646657  Wed Apr 08 09:18:39 +0000 2015   \n",
       "\n",
       "                                               tweet  \n",
       "0  Breast cancer risk test devised http://bbc.in/...  \n",
       "1  GP workload harming care - BMA poll http://bbc...  \n",
       "2  Short people's 'heart risk greater' http://bbc...  \n",
       "3  New approach against HIV 'promising' http://bb...  \n",
       "4  Coalition 'undermined NHS' - doctors http://bb...  \n",
       "5  Review of case against NHS manager http://bbc....  \n",
       "6  VIDEO: 'All day is empty, what am I going to d...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at the first 7 records in the dataframe\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7770</th>\n",
       "      <td>80654575278895105</td>\n",
       "      <td>Tue Jun 14 15:15:23 +0000 2011</td>\n",
       "      <td>15 healthy-eating tips for people with #Crohns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7771</th>\n",
       "      <td>80641899744067584</td>\n",
       "      <td>Tue Jun 14 14:25:01 +0000 2011</td>\n",
       "      <td>RT @Cooking_Light: Smart snack: Greek yogurt o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7772</th>\n",
       "      <td>80629372100415488</td>\n",
       "      <td>Tue Jun 14 13:35:14 +0000 2011</td>\n",
       "      <td>Living With Pets May Protect Infants From Alle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7773</th>\n",
       "      <td>80629263631527936</td>\n",
       "      <td>Tue Jun 14 13:34:49 +0000 2011</td>\n",
       "      <td>@SarahCampus We know the feeling! Here are som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7774</th>\n",
       "      <td>80390283203842048</td>\n",
       "      <td>Mon Jun 13 21:45:11 +0000 2011</td>\n",
       "      <td>The Simple Secret to Great Sleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7775</th>\n",
       "      <td>80371370009174016</td>\n",
       "      <td>Mon Jun 13 20:30:02 +0000 2011</td>\n",
       "      <td>Hot days call for refreshing drinks. Whip up t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7776</th>\n",
       "      <td>80352600800763904</td>\n",
       "      <td>Mon Jun 13 19:15:27 +0000 2011</td>\n",
       "      <td>RT @MSNHealth: The Mediterranean? The Volumetr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                        datetime  \\\n",
       "7770  80654575278895105  Tue Jun 14 15:15:23 +0000 2011   \n",
       "7771  80641899744067584  Tue Jun 14 14:25:01 +0000 2011   \n",
       "7772  80629372100415488  Tue Jun 14 13:35:14 +0000 2011   \n",
       "7773  80629263631527936  Tue Jun 14 13:34:49 +0000 2011   \n",
       "7774  80390283203842048  Mon Jun 13 21:45:11 +0000 2011   \n",
       "7775  80371370009174016  Mon Jun 13 20:30:02 +0000 2011   \n",
       "7776  80352600800763904  Mon Jun 13 19:15:27 +0000 2011   \n",
       "\n",
       "                                                  tweet  \n",
       "7770  15 healthy-eating tips for people with #Crohns...  \n",
       "7771  RT @Cooking_Light: Smart snack: Greek yogurt o...  \n",
       "7772  Living With Pets May Protect Infants From Alle...  \n",
       "7773  @SarahCampus We know the feeling! Here are som...  \n",
       "7774                  The Simple Secret to Great Sleep   \n",
       "7775  Hot days call for refreshing drinks. Whip up t...  \n",
       "7776  RT @MSNHealth: The Mediterranean? The Volumetr...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display the last 7 records in the dataframe\n",
    "df.tail(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and preparations for data cleaning\n",
    "### Before cleaning the data, install spacy (using neural networks for natural language processing)\n",
    "by running: \n",
    "`conda install spacy` \n",
    "`python -m spacy download en_core_web_sm`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy   \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of stop words from nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add additional stop words and save as a set\n",
    "stopwords = set(stopwords + ['RT', 'health', 'healthcare', 'video', 'rt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def clean_text(docs):\n",
    "    \"\"\"Performs lemmatization and removes both stopwords and punctuation from panda series with text\n",
    "       docs: pandas series with text\n",
    "    \"\"\"\n",
    "    # remove punctuation and numbers\n",
    "    print('removing punctuation and digits...')\n",
    "    table = str.maketrans({key: None for key in string.punctuation + string.digits})\n",
    "   \n",
    "    clean_docs = [d.translate(table) for d in docs]\n",
    "    \n",
    "    print('spacy nlp...')\n",
    "    # nlp() function uses spacy's neural network to preform lemmatization on the collection provided by caller\n",
    "    nlp_docs = [nlp(d) for d in clean_docs]\n",
    "    \n",
    "    # keep the word if it's a pronoun, otherwise use the lemma\n",
    "    # otherwise spacy substitutes '-PRON-' for pronouns\n",
    "    print('getting lemmas...')\n",
    "    lemmatized_docs = [[w.lemma_ if w.lemma_ != '-PRON-'   # use lemma unless it is '-PRON-'\n",
    "                           else w.lower_                   # lower case all words\n",
    "                           for w in d]                     # get each word in the document\n",
    "                      for d in nlp_docs]                   # get each spacy lemmatized document \n",
    "    \n",
    "    # remove stopwords\n",
    "    print('removing stopwords...')\n",
    "    lemmatized_docs = [[lemma for lemma in doc if lemma not in stopwords] for doc in lemmatized_docs]\n",
    "    \n",
    "    # join tokens back into a string with each word separated by a space\n",
    "    clean_docs = [' '.join(l) for l in lemmatized_docs]\n",
    "     \n",
    "    # return a reference to the list of strings   \n",
    "    return clean_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'datetime', 'tweet'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns #look up column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweets):\n",
    "    \"\"\" deletes URLs and user names from tweets\"\"\"\n",
    "    cleaned_tweet = tweets.apply(lambda x: re.sub('http://\\S+|@\\S+', '', x))\n",
    "    return cleaned_tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Breast cancer risk test devised http://bbc.in/...\n",
       "1    GP workload harming care - BMA poll http://bbc...\n",
       "2    Short people's 'heart risk greater' http://bbc...\n",
       "3    New approach against HIV 'promising' http://bb...\n",
       "4    Coalition 'undermined NHS' - doctors http://bb...\n",
       "5    Review of case against NHS manager http://bbc....\n",
       "6    VIDEO: 'All day is empty, what am I going to d...\n",
       "7    VIDEO: 'Overhaul needed' for end-of-life care ...\n",
       "8    Care for dying 'needs overhaul' http://bbc.in/...\n",
       "9    VIDEO: NHS: Labour and Tory key policies http:...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at the first 10 tweets before processing\n",
    "df['tweet'][:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_before = df['tweet'] #Retreive the tweets column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweets_before) #check the data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Breast cancer risk test devised http://bbc.in/...\n",
       "1    GP workload harming care - BMA poll http://bbc...\n",
       "2    Short people's 'heart risk greater' http://bbc...\n",
       "3    New approach against HIV 'promising' http://bb...\n",
       "4    Coalition 'undermined NHS' - doctors http://bb...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_before.head() #first five rows of uncleaned tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean= clean_tweet(tweets_before)    #call clean_tweet function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27738"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#length before cleaning\n",
    "len(tweets_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27738"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#length after deleting URLs and usernames\n",
    "len(tweets_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweets_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     Breast cancer risk test devised \n",
       "1                 GP workload harming care - BMA poll \n",
       "2                 Short people's 'heart risk greater' \n",
       "3                New approach against HIV 'promising' \n",
       "4                Coalition 'undermined NHS' - doctors \n",
       "5                  Review of case against NHS manager \n",
       "6    VIDEO: 'All day is empty, what am I going to d...\n",
       "7       VIDEO: 'Overhaul needed' for end-of-life care \n",
       "8                     Care for dying 'needs overhaul' \n",
       "9            VIDEO: NHS: Labour and Tory key policies \n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the results\n",
    "tweets_clean.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing punctuation and digits\n",
      "spacy nlp...\n",
      "getting lemmas...\n",
      "removing stopwords...\n"
     ]
    }
   ],
   "source": [
    "# Call the clean_text() function and pass the tweets column\n",
    "# This step might take a while to run .... coffee break....\n",
    "tweets_cleantext = clean_text(tweets_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['less sex',\n",
       " 'five idea transform nhs',\n",
       " 'personal cancer vaccine exciting',\n",
       " 'child heart surgery death halve',\n",
       " 'miliband cameron fail nhs',\n",
       " 'unsafe food grow global threat',\n",
       " 'highlight',\n",
       " 'ambulance progress fast enough',\n",
       " 'children√¢\\x80\\x99s hospital build sleep app',\n",
       " 'drug giant block eye treatment']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check tweets 20 through 29\n",
    "tweets_cleantext[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check output type\n",
    "type(tweets_cleantext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-standard characters by encoding and decoding basic ASCII characters\n",
    "tweets_textready=[x.encode('ascii', errors='ignore').decode('ascii') for x in tweets_cleantext ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['less sex',\n",
       " 'five idea transform nhs',\n",
       " 'personal cancer vaccine exciting',\n",
       " 'child heart surgery death halve',\n",
       " 'miliband cameron fail nhs',\n",
       " 'unsafe food grow global threat',\n",
       " 'highlight',\n",
       " 'ambulance progress fast enough',\n",
       " 'childrens hospital build sleep app',\n",
       " 'drug giant block eye treatment']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check results\n",
    "tweets_textready[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create sparse matrix\n",
    "vectorizer = TfidfVectorizer(min_df=2) #do not include words with count less than 2\n",
    "features = vectorizer.fit_transform(tweets_textready)\n",
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27738, 8632)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dimentions of the sparse matrix\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the sparce matix to a numpy dense matrix \n",
    "features = features.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a model with 2 clusters\n",
    "model = KMeans(n_clusters=2, random_state=123, n_jobs=-1) #n_jobs=-1 use all the cores, random state=123\n",
    "#makes it reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=2, n_init=10, n_jobs=-1, precompute_distances='auto',\n",
       "    random_state=123, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model fitting ... might take a while ...\n",
    "model.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# cluster the data into 2 to 9 clusters ( n_clusters - hyperparameter)\n",
    "# calculate wws - total within-cluster sum of squares to measure compactness (goodness of fit) of clusters\n",
    "# This step takes even longer to run\n",
    "wss = []\n",
    "for n in range(2, 10):\n",
    "    print(n)\n",
    "    model = KMeans(n_clusters=n, random_state=42, n_jobs=-1)\n",
    "    model.fit(features)\n",
    "    wss.append(-model.score(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the optimum number of clusters using the elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2c04d786630>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(2, 10), wss) # plot wss values for 2-9 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-94.72014014, -44.33652431, -87.80380574, -69.31324813,\n",
       "       -65.45245162, -13.98667859, -87.32801395])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find relative changes in wss for 2-9 clusters \n",
    "np.diff(wss) #differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27485.08319004147,\n",
       " 27390.36304990611,\n",
       " 27346.026525593792,\n",
       " 27258.222719857356,\n",
       " 27188.909471726878,\n",
       " 27123.457020106754,\n",
       " 27109.470341519827,\n",
       " 27022.142327571783]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wss #print absolute values for wss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of applying the elbow method to find an optimum number of clusters for this dataset are inconclusive. There is no clear pivot point on the wss graph. An alternative method - the silhoutte method from skelen.metrics theat measures the quality of the clustering did not provide any consistent results either. So, for the purposes of this assignment I chose the number of clusters = 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=5, n_init=10, n_jobs=-1, precompute_distances='auto',\n",
       "    random_state=123, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model with 5 clusters\n",
    "model_5 = KMeans(n_clusters=5, random_state=123, n_jobs=-1)\n",
    "#Model fitting ... \n",
    "model_5.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions\n",
    "cluster_labels = model_5.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 1, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample output cluster labels\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2285,  2287,  1075,  1669, 20422], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see resulting distribution of tweets by clusters\n",
    "np.bincount(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['gp workload harm care   bma poll', 'new approach hiv promise',\n",
       "       'day empty go', ..., 'simple secret great sleep',\n",
       "       'hot day call refreshing drink whip family figurefriendly sip sweet way cool',\n",
       "       '  mediterranean volumetrics maybe dash us news best overall diet plan  '],\n",
       "      dtype='<U5614')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract tweets that are grouped in cluster #4 \n",
    "cluster4_tweets = np.array(tweets_textready)[cluster_labels == 4]\n",
    "cluster4_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate word frequencies for the cluster #4\n",
    "wordcount = nltk.FreqDist(nltk.tokenize.word_tokenize(' '.join(cluster4_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('get', 1236),\n",
       " ('good', 881),\n",
       " ('say', 864),\n",
       " ('new', 840),\n",
       " ('make', 831),\n",
       " ('us', 747),\n",
       " ('healthtalk', 669),\n",
       " ('help', 667),\n",
       " ('day', 632),\n",
       " ('q', 623)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the top 10 words from the cluster #4\n",
    "wordcount.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['coalition undermine nhs   doctor', 'review case nhs manager',\n",
       "       'nhs labour tory key policy', ...,\n",
       "       'nhs staff go strike monday   nhsstrike',\n",
       "       'lord darzis plan improve londoners   nhs',\n",
       "       'big challenge face department also nhs whole lack money'],\n",
       "      dtype='<U5614')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract tweets for the cluster # 3\n",
    "cluster3_tweets = np.array(tweets_textready)[cluster_labels == 3]\n",
    "cluster3_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nhs', 1747),\n",
       " ('new', 175),\n",
       " ('miss', 164),\n",
       " ('patient', 126),\n",
       " ('need', 123),\n",
       " ('today', 119),\n",
       " ('care', 113),\n",
       " ('say', 101),\n",
       " ('staff', 99),\n",
       " ('work', 88)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate word frequencies for the cluster #3\n",
    "wordcount3 = nltk.FreqDist(nltk.tokenize.word_tokenize(' '.join(cluster3_tweets)))\n",
    "# get the top 10 words from the cluster #3\n",
    "wordcount3.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['guinea declare ebola emergency', 'military worker free ebola',\n",
       "       'british medic declare free ebola', ...,\n",
       "       'ebola ice bucket challenge mosttalkedabout facebook topic for  ',\n",
       "       '  ebola fighters times person year    timepoy',\n",
       "       'ebola patient dr martin salia dies nebraska hospital   ebola'],\n",
       "      dtype='<U5614')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract tweets for the cluster # 2\n",
    "cluster2_tweets = np.array(tweets_textready)[cluster_labels == 2]\n",
    "cluster2_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ebola', 1088),\n",
       " ('outbreak', 141),\n",
       " ('say', 113),\n",
       " ('sierra', 94),\n",
       " ('leone', 92),\n",
       " ('patient', 92),\n",
       " ('vaccine', 90),\n",
       " ('case', 89),\n",
       " ('us', 80),\n",
       " ('liberia', 73)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate word frequencies for the cluster #2\n",
    "wordcount2 = nltk.FreqDist(nltk.tokenize.word_tokenize(' '.join(cluster2_tweets)))\n",
    "# get the top 10 words from the cluster #2\n",
    "wordcount2.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['breast cancer risk test devise', 'short people heart risk great',\n",
       "       'strenuous run may bad', ...,\n",
       "       '  sound like may enjoy good fatburning breakfast',\n",
       "       'toss turn may hurt marriage',\n",
       "       'live pets may protect infants allergies'], dtype='<U5614')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract tweets for the cluster # 1\n",
    "cluster1_tweets = np.array(tweets_textready)[cluster_labels == 1]\n",
    "cluster1_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('may', 1109),\n",
       " ('cancer', 854),\n",
       " ('risk', 508),\n",
       " ('study', 212),\n",
       " ('help', 183),\n",
       " ('say', 148),\n",
       " ('new', 132),\n",
       " ('breast', 121),\n",
       " ('find', 108),\n",
       " ('heart', 105)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate word frequencies for the cluster #1\n",
    "wordcount1 = nltk.FreqDist(nltk.tokenize.word_tokenize(' '.join(cluster1_tweets)))\n",
    "# get the top 10 words from the cluster #1\n",
    "wordcount1.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['unsafe food grow global threat', 'make healthy high street',\n",
       "       'dr radhas guide healthy diet', ...,\n",
       "       '  eat healthy accord myplate guideline less   week',\n",
       "       'hurry hungry heres eat stay slim go courtesy  ',\n",
       "       'enjoy summer strawberri straight carton shortcake pie healthy recipe'],\n",
       "      dtype='<U5614')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract tweets for the cluster # 0\n",
    "cluster0_tweets = np.array(tweets_textready)[cluster_labels == 0]\n",
    "cluster0_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('food', 1065),\n",
       " ('healthy', 927),\n",
       " ('eat', 689),\n",
       " ('q', 256),\n",
       " ('make', 187),\n",
       " ('good', 180),\n",
       " ('get', 174),\n",
       " ('amp', 166),\n",
       " ('healthtalk', 150),\n",
       " ('recipe', 128)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate word frequencies for the cluster #0\n",
    "wordcount0 = nltk.FreqDist(nltk.tokenize.word_tokenize(' '.join(cluster0_tweets)))\n",
    "# get the top 10 words from the cluster #0\n",
    "wordcount0.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling Using Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVD - vector representation of documents and terms\n",
    "#n_components parameter specifies the number of topics\n",
    "#since I originally used 8 files  => n_components=8\n",
    "svd_model = TruncatedSVD(n_components = 8, algorithm='randomized', n_iter = 100, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=8, n_iter=100,\n",
       "       random_state=123, tol=0.0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the SVD model to the document-term matrix\n",
    "svd_model.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Topic 0:\n",
      "nhs get good food new may make help healthy way \n",
      " Topic 1:\n",
      "nhs ebola patient new say cancer care miss doctor hospital \n",
      " Topic 2:\n",
      "nhs day recipe today get miss try care viewsfromthenhsfrontline work \n",
      " Topic 3:\n",
      "day ebola recipe get us try outbreak healthy vaccine leone \n",
      " Topic 4:\n",
      "ebola weight lose nhs food eat outbreak loss diet pound \n",
      " Topic 5:\n",
      "weight lose day recipe help may loss try diet way \n",
      " Topic 6:\n",
      "food eat good life nhs ebola bad day long recipe \n",
      " Topic 7:\n",
      "get good way bad workout exercise fitness move ebola via "
     ]
    }
   ],
   "source": [
    "#print the most important words for each topic\n",
    "terms=vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key=lambda x:x[1], reverse=True)[:10]\n",
    "    print('\\n',\"Topic \"+str(i)+\":\")\n",
    "    for t in sorted_terms:\n",
    "        print(t[0], end=' ')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "1. Latent Semantic Analysis Introductory Tutorial: [ww.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/](https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/)\n",
    "\n",
    "2. KMeans Tutorial: [www.datascience.com/blog/k-means-clustering](https://www.datascience.com/blog/k-means-clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
